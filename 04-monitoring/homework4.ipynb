{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049479a9-aa07-4fc4-9e89-d1358f76deab",
   "metadata": {},
   "source": [
    "<center><H2>Homework 4</H2></center>\n",
    "<center><H4>Date: Aug 10, 2024</H4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f1a46-af5e-4fcd-b1d1-8d82e75577a1",
   "metadata": {},
   "source": [
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38acee89-3996-4b3f-93f2-ddd435f7977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "import pprint\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d0ff2d-8752-4646-8e94-2e04c39cce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88447e54-aaad-47f3-bdba-41ef34b72946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:300]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b142495-2254-4911-b678-09a58b0a79f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_llm</th>\n",
       "      <th>answer_orig</th>\n",
       "      <th>document</th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You can sign up for the course by visiting the...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Where can I sign up for the course?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You can sign up using the link provided in the...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Can you provide a link to sign up?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes, there is an FAQ for the Machine Learning ...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Is there an FAQ for this Machine Learning course?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The context does not provide any specific info...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Does this course have a GitHub repository for ...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To structure your questions and answers for th...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>How can I structure my questions and answers f...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          answer_llm  \\\n",
       "0  You can sign up for the course by visiting the...   \n",
       "1  You can sign up using the link provided in the...   \n",
       "2  Yes, there is an FAQ for the Machine Learning ...   \n",
       "3  The context does not provide any specific info...   \n",
       "4  To structure your questions and answers for th...   \n",
       "\n",
       "                                         answer_orig  document  \\\n",
       "0  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "1  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "2  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "3  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "4  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "\n",
       "                                            question  \\\n",
       "0                Where can I sign up for the course?   \n",
       "1                 Can you provide a link to sign up?   \n",
       "2  Is there an FAQ for this Machine Learning course?   \n",
       "3  Does this course have a GitHub repository for ...   \n",
       "4  How can I structure my questions and answers f...   \n",
       "\n",
       "                      course  \n",
       "0  machine-learning-zoomcamp  \n",
       "1  machine-learning-zoomcamp  \n",
       "2  machine-learning-zoomcamp  \n",
       "3  machine-learning-zoomcamp  \n",
       "4  machine-learning-zoomcamp  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f293eb4-49ef-4814-b031-85cea8bc822c",
   "metadata": {},
   "source": [
    "## Q1. Getting the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85448735-cdf7-4dc9-9e2c-ff8a583735f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda7bcd8-197c-47f0-83a4-8e565b56ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers     3.0.1\n",
      "transformers              4.42.4\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8bf43-c5b3-4a72-8252-079fb0ae0034",
   "metadata": {},
   "source": [
    "### Embedding for the first LLM answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bf1b24-c1e7-4ac9-b5fc-390218df2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee16d4ab-7d6f-4345-aade-21dcfffdb21b",
   "metadata": {},
   "outputs": [],
   "source": [
    " answer_llm_embedding = model.encode(answer_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c12de5f1-2c57-4fff-9d7c-fddf8ea79b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42244682"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_llm_embedding[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f02ac-8af7-4870-9cfd-eab64e79ae87",
   "metadata": {},
   "source": [
    "#### Example dot product of two embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae096bb-478c-47fa-b8ac-d132b7f9d128",
   "metadata": {},
   "source": [
    "#### Iterative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8487252-79da-452a-ba45-e8d90b6bf4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "10 s ± 275 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def iterative_approach(df, model):\n",
    "    evaluations = []\n",
    "    for index, row in df.iterrows():\n",
    "        embedding_llm = model.encode(row['answer_llm'])\n",
    "        embedding_orig = model.encode(row['answer_orig'])\n",
    "        score = np.dot(embedding_llm, embedding_orig)\n",
    "        evaluations.append(score)\n",
    "        \n",
    "    percentile_75 = np.percentile(dot_products, 75) \n",
    "    print(f\"75th percentile of the scores: {round(percentile_75,2)}\")\n",
    "    \n",
    "    return percentile_75\n",
    "\n",
    "# Measure runtime for the iterative approach\n",
    "%timeit iterative_approach(df, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb1fc2-1f11-424d-9ef8-ac7fbac1c378",
   "metadata": {},
   "source": [
    "#### Vectorized approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28f784a3-afda-4cd7-951e-7c7b1ab47f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "75th percentile of the scores: 31.67\n",
      "6.52 s ± 18.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def vectorized_approach(df, model):\n",
    "    embeddings_llm = model.encode(df['answer_llm'].tolist(), convert_to_tensor=True)\n",
    "    embeddings_orig = model.encode(df['answer_orig'].tolist(), convert_to_tensor=True)\n",
    "    dot_products = (embeddings_llm * embeddings_orig).sum(dim=1).cpu().numpy()\n",
    "\n",
    "    percentile_75 = np.percentile(dot_products, 75) \n",
    "    print(f\"75th percentile of the scores: {round(percentile_75,2)}\")\n",
    "    \n",
    "    return percentile_75\n",
    "\n",
    "# Measure runtime for the vectorized approach\n",
    "%timeit vectorized_approach(df, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14afa6-b01f-44ac-b146-32efe4176ffe",
   "metadata": {},
   "source": [
    "### Q3. Computing the cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69ee4d-a4ba-4346-9ed5-7719aa7cc930",
   "metadata": {},
   "source": [
    "#### Vector NORM calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbdc257-51b7-401a-a276-0f475926e438",
   "metadata": {},
   "source": [
    "norm = np.sqrt((v * v).sum())\n",
    "\n",
    "v_norm = v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1962ac63-c05c-48b6-8bed-e229654f802e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([1,2,3])\n",
    "norm = np.sqrt((v * v).sum())\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d81f96f0-ddcc-4142-b730-3be1e80375eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26726124, 0.53452248, 0.80178373])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_norm = v / norm\n",
    "v_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4695c61-61af-4ab6-8ffc-c87dba6cec35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26726124, 0.53452248, 0.80178373])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_norm_vector(v):\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    v_norm = v / norm\n",
    "    return v_norm\n",
    "\n",
    "get_norm_vector(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8feca9a7-207a-48f2-b79c-db2e714ed3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75th percentile of the scores: 0.836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.836"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iterative_approach(df, model):\n",
    "    evaluations = []\n",
    "    for index, row in df.iterrows():\n",
    "        embedding_llm = get_norm_vector(model.encode(row['answer_llm']))\n",
    "        embedding_orig = get_norm_vector(model.encode(row['answer_orig']))\n",
    "        score = np.dot(embedding_llm, embedding_orig)\n",
    "        evaluations.append(score)\n",
    "        \n",
    "    percentile_75 = np.percentile(evaluations, 75) \n",
    "    print(f\"75th percentile of the scores: {round(percentile_75,3)}\")\n",
    "    \n",
    "    return round(percentile_75,3)\n",
    "\n",
    "# Measure runtime for the iterative approach\n",
    "iterative_approach(df, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bbc5c2-2e9e-4c6e-97c8-b3d3a51e3892",
   "metadata": {},
   "source": [
    "### Q4. Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f97716af-e185-4ff6-b717-ee6d39de38df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /Users/manaschaity/Documents/python_venvs/venv_llmzc/lib/python3.12/site-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04aeb262-92ba-4ae1-8a3c-23fd9bac8ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.45454544954545456,\n",
      "             'p': 0.45454545454545453,\n",
      "             'r': 0.45454545454545453},\n",
      " 'rouge-2': {'f': 0.21621621121621637,\n",
      "             'p': 0.21621621621621623,\n",
      "             'r': 0.21621621621621623},\n",
      " 'rouge-l': {'f': 0.393939388939394,\n",
      "             'p': 0.3939393939393939,\n",
      "             'r': 0.3939393939393939}}\n"
     ]
    }
   ],
   "source": [
    "rouge_scorer = Rouge()\n",
    "\n",
    "df_i10 = df.iloc[10]\n",
    "\n",
    "scores = rouge_scorer.get_scores(df_i10['answer_llm'], df_i10['answer_orig'])[0]\n",
    "pprint.pprint(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ef1a598-d961-49eb-9f1e-d2360a904c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.455"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(scores['rouge-1']['f'],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd775d-d69e-471f-bf5d-9b89b4aefe48",
   "metadata": {},
   "source": [
    "### Q5. Average rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b7385d4a-dfcb-4ca8-adb0-b5bd92554cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average F-score between rouge-1, rouge-2 and rouge-l: 0.355\n"
     ]
    }
   ],
   "source": [
    "r1 = round(scores['rouge-1']['f'],3)\n",
    "r2 = round(scores['rouge-2']['f'],3)\n",
    "rl = round(scores['rouge-l']['f'],3)\n",
    "\n",
    "\n",
    "print(f'the average F-score between rouge-1, rouge-2 and rouge-l: { (r1+r2+rl)/3 }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2df7a-1c35-4282-ad18-582177ec83d5",
   "metadata": {},
   "source": [
    "### Q6. Average rouge score for all the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "976acd0b-41ed-44de-8bca-ab141e998d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f-score in rought-2 for the dataset: 0.207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.207"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_rough_score(df_param):\n",
    "    rouge_2_f_scores = []\n",
    "    for index, row in df_param.iterrows():\n",
    "        scores = rouge_scorer.get_scores(row['answer_llm'], row['answer_orig'])[0]\n",
    "\n",
    "        rouge_2_f = scores['rouge-2']['f']\n",
    "        rouge_2_f_scores.append(rouge_2_f)\n",
    "        \n",
    "    average = statistics.mean(rouge_2_f_scores)\n",
    "    print(f\"Average f-score in rought-2 for the dataset: {round(average,3)}\")\n",
    "    \n",
    "    return round(average,3)\n",
    "\n",
    "# Measure rouge score \n",
    "avg_rough_score(df) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llmzc",
   "language": "python",
   "name": "venv_llmzc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
